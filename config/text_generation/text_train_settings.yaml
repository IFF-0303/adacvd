condor:
  bid: 36
  memory_gpus: 51000
  request_cpus: 8
  request_gpus: 2
  request_memory: 300000
data:
  filter_eos: false
  num_training_samples: 20000
  eval_subset: true
  feature_config:
    feature_groups:
      - base_risk_score_inputs
      - additional_risk_score_inputs_aha_acc
      - additional_risk_score_inputs_prevent
  oversampled_pos_fraction: 0.0
  path: /fast/groups/hfm-users/pandora-med-box/results/2025_02_26_text_dataset_full_mistral_20k/text_generation_output.csv
  shuffle: false
  split: config/splits/split_2025_02_17_test_100000.json
  subset: subsets/ukb_2024_02/MACE_ADO_EXTENDED_no_previous_target.json
  target_name: MACE_ADO_EXTENDED_10y
  text_generation_prompt: true
  zeroshot_prompt: false
model:
  autocast_adapter_dtype: false
  finetuning:
    lora_dropout: 0.01
    r: 16
    target_modules:
      - q_proj
      - k_proj
      - v_proj
    type: lora
  model_checkpoint: /fast/groups/hfm-users/pandora-med-box/results/2025_03_06_mistral_fgs/015/checkpoint_2_n450000
  name: mistralai/Mistral-7B-Instruct-v0.3
  precision: 16b
  resume_training: true
  type: huggingface
training:
  batch_size: 16
  checkpoint_steps: 400000
  epochs: 5
  eval_batch_size: 4
  eval_steps: 300
  extended_evaluation: false
  full_eval_step_n: 1000000
  mask_labels: true
  num_workers: 4
  optimizer:
    lr: 0.0001
    type: adamw
  random_seed: 0
  save_model: false
  scheduler:
    gamma: 0.75
    step_size: 1250
    type: step
